{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphsaint.sampler import SAINTNodeSampler, SAINTEdgeSampler, SAINTRandomWalkSampler\n",
    "from graphsaint.config import CONFIG\n",
    "from graphsaint.modules import GCNNet\n",
    "from graphsaint.utils import Logger, evaluate, save_log_dir, load_data, calc_f1\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Juyeong.aug import HLoss, Jensen_Shannon, generate_aug_graph"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "graphsaint > tran_sampling.py > main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {\n",
    "        'aggr': 'concat', 'arch': '1-0-1-0', 'dataset': 'ppi', 'dropout': 0, 'edge_budget': 4000, 'length': 2,\n",
    "        'log_dir': 'none', 'lr': 0.005, 'decay': 0.0005, 'n_epochs': 50, 'n_hidden': 512, 'no_batch_norm': False, 'node_budget': 6000,\n",
    "        'num_subg': 50, 'num_roots': 3000, 'sampler': 'node', 'use_val': True, 'val_every': 1, 'num_workers_sampler': 0,\n",
    "        'num_subg_sampler': 10000, 'batch_size_sampler': 200, 'num_workers': 8, 'full': True,\n",
    "        'sigma_delta_e': 0.03, 'sigma_delta_v': 0.03, 'mu_e': 0.6, 'mu_v': 0.2, 'lam1_e': 1, 'lam1_v': 1, 'lam2_e': 0.0, 'lam2_v': 0.0,\n",
    "        'a_e': 100, 'b_e': 1, 'a_v': 100, 'b_v': 1, 'kl': 2.0, 'h': 0.2, 'online': False, 'gpu': 0,'task': 'ppi_n'\n",
    "}\n",
    "multilabel =  True\n",
    "\n",
    "from collections import namedtuple\n",
    "A = namedtuple('a', a)\n",
    "args = A(**a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and preprocess dataset\n",
    "data = load_data(args, multilabel)\n",
    "g = data.g\n",
    "train_mask = g.ndata['train_mask']\n",
    "val_mask = g.ndata['val_mask']\n",
    "test_mask = g.ndata['test_mask']\n",
    "labels = g.ndata['label']\n",
    "\n",
    "train_nid = data.train_nid\n",
    "\n",
    "in_feats = g.ndata['feat'].shape[1]\n",
    "n_classes = data.num_classes\n",
    "n_nodes = g.num_nodes()\n",
    "n_edges = g.num_edges()\n",
    "\n",
    "n_train_samples = train_mask.int().sum().item()\n",
    "n_val_samples = val_mask.int().sum().item()\n",
    "n_test_samples = test_mask.int().sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(num_classes=121, train_nid=array([   0,    1,    2, ..., 9713, 9714, 9715]), g=Graph(num_nodes=14755, num_edges=450540,\n",
       "      ndata_schemes={'feat': Scheme(shape=(50,), dtype=torch.float32), 'label': Scheme(shape=(121,), dtype=torch.float32), 'train_mask': Scheme(shape=(), dtype=torch.bool), 'val_mask': Scheme(shape=(), dtype=torch.bool), 'test_mask': Scheme(shape=(), dtype=torch.bool)}\n",
       "      edata_schemes={}))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "        'dn': args.dataset, 'g': g, 'train_nid': train_nid, 'num_workers_sampler': args.num_workers_sampler,\n",
    "        'num_subg_sampler': args.num_subg_sampler, 'batch_size_sampler': args.batch_size_sampler,\n",
    "        'online': args.online, 'num_subg': args.num_subg, 'full': args.full\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.1428573 2.9642859 3.        ... 3.        3.2807019 3.1612902]\n",
      "[0.00028198 0.00228718 0.00027085 ... 0.00036113 0.00128654 0.00033201]\n",
      "The number of subgraphs is:  200\n"
     ]
    }
   ],
   "source": [
    "if args.sampler == \"node\":\n",
    "        saint_sampler = SAINTNodeSampler(args.node_budget, **kwargs)\n",
    "elif args.sampler == \"edge\":\n",
    "    saint_sampler = SAINTEdgeSampler(args.edge_budget, **kwargs)\n",
    "elif args.sampler == \"rw\":\n",
    "    saint_sampler = SAINTRandomWalkSampler(args.num_roots, args.length, **kwargs)\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "loader = DataLoader(saint_sampler, collate_fn=saint_sampler.__collate_fn__, batch_size=1,\n",
    "                    shuffle=True, num_workers=args.num_workers, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels shape: torch.Size([14755, 121])\n",
      "features shape: torch.Size([14755, 50])\n",
      "a(aggr='concat', arch='1-0-1-0', dataset='ppi', dropout=0, edge_budget=4000, length=2, log_dir='none', lr=0.005, decay=0.0005, n_epochs=50, n_hidden=512, no_batch_norm=False, node_budget=6000, num_subg=50, num_roots=3000, sampler='node', use_val=True, val_every=1, num_workers_sampler=0, num_subg_sampler=10000, batch_size_sampler=200, num_workers=8, full=True, sigma_delta_e=0.03, sigma_delta_v=0.03, mu_e=0.6, mu_v=0.2, lam1_e=1, lam1_v=1, lam2_e=0.0, lam2_v=0.0, a_e=100, b_e=1, a_v=100, b_v=1, kl=2.0, h=0.2, online=False, gpu=0, task='ppi_n')\n",
      "GPU memory allocated before training(MB) 26.75830078125\n"
     ]
    }
   ],
   "source": [
    "# set device for dataset tensors\n",
    "cpu_flag = False\n",
    "\n",
    "if args.gpu < 0:\n",
    "    cuda = False\n",
    "else:\n",
    "    cuda = True\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "    val_mask = val_mask.cuda()\n",
    "    test_mask = test_mask.cuda()\n",
    "    if not cpu_flag:\n",
    "        g = g.to('cuda:{}'.format(args.gpu))\n",
    "\n",
    "print('labels shape:', g.ndata['label'].shape)\n",
    "print(\"features shape:\", g.ndata['feat'].shape)\n",
    "\n",
    "model = GCNNet(\n",
    "    in_dim=in_feats,\n",
    "    hid_dim=args.n_hidden,\n",
    "    out_dim=n_classes,\n",
    "    arch=args.arch,\n",
    "    dropout=args.dropout,\n",
    "    batch_norm=not args.no_batch_norm,\n",
    "    aggr=args.aggr\n",
    ")\n",
    "\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "\n",
    "# logger and so on\n",
    "log_dir = save_log_dir(args)\n",
    "logger = Logger(os.path.join(log_dir, 'loggings'))\n",
    "logger.write(args)\n",
    "\n",
    "# use optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                lr=args.lr)\n",
    "\n",
    "# set train_nids to cuda tensor\n",
    "if cuda:\n",
    "    train_nid = torch.from_numpy(train_nid).cuda()\n",
    "    print(\"GPU memory allocated before training(MB)\",\n",
    "            torch.cuda.memory_allocated(device=train_nid.device) / 1024 / 1024)\n",
    "start_time = time.time()\n",
    "best_f1 = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# epoch:  0\n",
      "# j:  0\n",
      "Subg original tensor([[1.]], device='cuda:0')\n",
      "Subg aug tensor([[10.]], device='cuda:0')\n",
      "-------\n",
      "# epoch:  0\n",
      "# j:  1\n",
      "Subg original tensor([[2.]], device='cuda:0')\n",
      "Subg aug tensor([[10.],\n",
      "        [20.]], device='cuda:0')\n",
      "-------\n",
      "# epoch:  0\n",
      "# j:  2\n",
      "Subg original tensor([[3.]], device='cuda:0')\n",
      "Subg aug tensor([[10.],\n",
      "        [20.],\n",
      "        [30.]], device='cuda:0')\n",
      "-------\n",
      "# epoch:  1\n",
      "# j:  0\n",
      "Subg original tensor([[10.],\n",
      "        [20.],\n",
      "        [30.]], device='cuda:0')\n",
      "Subg aug tensor([[10.]], device='cuda:0')\n",
      "-------\n",
      "# epoch:  1\n",
      "# j:  1\n",
      "Subg original tensor([[10.]], device='cuda:0')\n",
      "Subg aug tensor([[10.],\n",
      "        [20.]], device='cuda:0')\n",
      "-------\n",
      "# epoch:  1\n",
      "# j:  2\n",
      "Subg original tensor([[10.],\n",
      "        [20.]], device='cuda:0')\n",
      "Subg aug tensor([[10.],\n",
      "        [20.],\n",
      "        [30.]], device='cuda:0')\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "data = torch.Tensor([[1], [2], [3]]).to(torch.cuda.current_device())\n",
    "d = DataLoader(data)\n",
    "\n",
    "def _augment(g):\n",
    "    return g*10\n",
    "\n",
    "# subg_original = torch.empty(1).to(torch.cuda.current_device())\n",
    "# subg_aug = torch.empty(1).to(torch.cuda.current_device())\n",
    "\n",
    "for epoch in range(2):\n",
    "    for j, subg in enumerate(d):\n",
    "        if epoch == 0:\n",
    "            subg_original = subg#.clone()\n",
    "        else:\n",
    "            # subg_original = torch.cat((subg_original, subg))\n",
    "            subg_original = subg_aug#.clone()\n",
    "\n",
    "        if j == 0:\n",
    "            subg_aug = _augment(subg)      # simulate augmentation\n",
    "        else:\n",
    "            subg_aug = torch.cat((subg_aug, _augment(subg)))\n",
    "\n",
    "        print(\"# epoch: \", epoch)\n",
    "        print(\"# j: \", j)\n",
    "        print(\"Subg original\", subg_original)\n",
    "        print(\"Subg aug\", subg_aug)\n",
    "        print(\"-------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Graph(num_nodes=4, num_edges=4,\n",
      "      ndata_schemes={}\n",
      "      edata_schemes={}), Graph(num_nodes=8, num_edges=4,\n",
      "      ndata_schemes={}\n",
      "      edata_schemes={})] []\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "\n",
    "u, v = torch.tensor([0, 0, 0, 1]), torch.tensor([1, 2, 3, 3])\n",
    "u2, v2 = torch.tensor([1, 2, 3, 4]), torch.tensor([4, 5, 6, 7])\n",
    "g = dgl.graph((u, v)).to(torch.cuda.current_device())\n",
    "g2 = dgl.graph((u2, v2)).to(torch.cuda.current_device())\n",
    "\n",
    "a = [g, g2]\n",
    "b = a\n",
    "a = []\n",
    "print(b, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Laurelwoods\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dgl\\backend\\pytorch\\tensor.py:445: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 32\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     29\u001b[0m     target \u001b[39m=\u001b[39m prev_aug[j]\n\u001b[0;32m     31\u001b[0m auged_subg, delta_G_e, delta_G_v, delta_G_e_aug, delta_G_v_aug \\\n\u001b[1;32m---> 32\u001b[0m     \u001b[39m=\u001b[39m generate_aug_graph(target, model,\n\u001b[0;32m     33\u001b[0m                             args\u001b[39m.\u001b[39;49msigma_delta_e, args\u001b[39m.\u001b[39;49msigma_delta_v, args\u001b[39m.\u001b[39;49mmu_e, args\u001b[39m.\u001b[39;49mmu_v,\n\u001b[0;32m     34\u001b[0m                             args\u001b[39m.\u001b[39;49mlam1_e, args\u001b[39m.\u001b[39;49mlam1_v, args\u001b[39m.\u001b[39;49mlam2_e, args\u001b[39m.\u001b[39;49mlam2_v,\n\u001b[0;32m     35\u001b[0m                             args\u001b[39m.\u001b[39;49ma_e, args\u001b[39m.\u001b[39;49mb_e, args\u001b[39m.\u001b[39;49ma_v, args\u001b[39m.\u001b[39;49mb_v)\n\u001b[0;32m     37\u001b[0m \u001b[39mif\u001b[39;00m j \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     38\u001b[0m     current_aug \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mf:\\Laurelwoods IDE\\GraphSaint-with-MH_Aug\\Juyeong\\aug.py:104\u001b[0m, in \u001b[0;36mgenerate_aug_graph\u001b[1;34m(g, model, sigma_delta_e, sigma_delta_v, mu_e, mu_v, lam1_e, lam1_v, lam2_e, lam2_v, a_e, b_e, a_v, b_v)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[39m# Graph Augmentation According To Delta Value\u001b[39;00m\n\u001b[0;32m    103\u001b[0m aug_g, aug_n_list \u001b[39m=\u001b[39m augment(g, delta_G_e_aug, delta_G_v_aug)\n\u001b[1;32m--> 104\u001b[0m aug_g \u001b[39m=\u001b[39m dgl\u001b[39m.\u001b[39;49madd_self_loop(aug_g)\n\u001b[0;32m    106\u001b[0m \u001b[39m# message_passing_g = copy.deepcopy(g)\u001b[39;00m\n\u001b[0;32m    107\u001b[0m message_passing_g \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39mclone()\n",
      "File \u001b[1;32mc:\\Users\\Laurelwoods\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dgl\\transforms\\functional.py:1881\u001b[0m, in \u001b[0;36madd_self_loop\u001b[1;34m(g, edge_feat_names, fill_data, etype)\u001b[0m\n\u001b[0;32m   1879\u001b[0m dtype \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39medges[etype]\u001b[39m.\u001b[39mdata[feat_name]\u001b[39m.\u001b[39mdtype\n\u001b[0;32m   1880\u001b[0m dshape \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39medges[etype]\u001b[39m.\u001b[39mdata[feat_name]\u001b[39m.\u001b[39mshape\n\u001b[1;32m-> 1881\u001b[0m tmp_fill_data \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mcopy_to(F\u001b[39m.\u001b[39;49mastype(F\u001b[39m.\u001b[39;49mtensor([fill_data]), dtype), g\u001b[39m.\u001b[39;49mdevice)\n\u001b[0;32m   1882\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(dshape) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1883\u001b[0m     data[feat_name] \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mzeros((g\u001b[39m.\u001b[39mnum_nodes(etype[\u001b[39m0\u001b[39m]), \u001b[39m*\u001b[39mdshape[\u001b[39m1\u001b[39m:]), dtype,\n\u001b[0;32m   1884\u001b[0m                               g\u001b[39m.\u001b[39mdevice) \u001b[39m+\u001b[39m tmp_fill_data\n",
      "File \u001b[1;32mc:\\Users\\Laurelwoods\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dgl\\backend\\pytorch\\tensor.py:142\u001b[0m, in \u001b[0;36mcopy_to\u001b[1;34m(input, ctx, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[39mif\u001b[39;00m ctx\u001b[39m.\u001b[39mindex \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    141\u001b[0m         th\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mset_device(ctx\u001b[39m.\u001b[39mindex)\n\u001b[1;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49mcuda(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid context\u001b[39m\u001b[39m\"\u001b[39m, ctx)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# subg_t = [[] for _ in range(2)]\n",
    "# subg_t = torch.empty((2, 1), dtype=torch.int32).to(torch.cuda.current_device())\n",
    "\n",
    "h_loss_op = HLoss()\n",
    "js_loss_op = Jensen_Shannon()\n",
    "\n",
    "current_aug = []\n",
    "\n",
    "for epoch in range(args.n_epochs):\n",
    "    if epoch == 3:\n",
    "        break ##########\n",
    "\n",
    "\n",
    "    if epoch > 0:\n",
    "        prev_aug = current_aug          \n",
    "\n",
    "    for j, subg in enumerate(loader):\n",
    "        if j == 2:\n",
    "            break ###############\n",
    "\n",
    "\n",
    "        if cuda:\n",
    "            subg = subg.to(torch.cuda.current_device())\n",
    "        # Augment Subgraph\n",
    "\n",
    "        if epoch == 0:\n",
    "            target = subg\n",
    "        else:\n",
    "            target = prev_aug[j]\n",
    "\n",
    "        auged_subg, delta_G_e, delta_G_v, delta_G_e_aug, delta_G_v_aug \\\n",
    "            = generate_aug_graph(target, model,\n",
    "                                    args.sigma_delta_e, args.sigma_delta_v, args.mu_e, args.mu_v,\n",
    "                                    args.lam1_e, args.lam1_v, args.lam2_e, args.lam2_v,\n",
    "                                    args.a_e, args.b_e, args.a_v, args.b_v)\n",
    "        \n",
    "        if j == 0:\n",
    "            current_aug = []\n",
    "        current_aug.append(auged_subg)#.clone()\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
