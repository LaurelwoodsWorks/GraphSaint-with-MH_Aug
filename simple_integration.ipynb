{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdb\n",
    "import random\n",
    "import datetime\n",
    "import argparse\n",
    "import json\n",
    "from collections import namedtuple\n",
    "import scipy.sparse\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import dgl\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.utils as u\n",
    "\n",
    "from mh_aug.utils import n_f1_score\n",
    "from mh_aug.model import GCN, MLP, GAT, SAGE, AGG_NET\n",
    "from mh_aug.preprocess_nc import load_nodes\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import truncnorm\n",
    "from scipy.special import betaln\n",
    "\n",
    "import torch.autograd.profiler as profiler\n",
    "\n",
    "import pickle\n",
    "import wandb\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model/Train loop from MH-Aug"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main.py > main()\n",
    "-> epi.py > episode()\n",
    "-> train()\n",
    "-> augment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logging_time(original_fn):\n",
    "    def wrapper_fn(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = original_fn(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(\"WorkingTime[{}]: {} sec\".format(original_fn.__name__, (end_time-start_time)*100))\n",
    "        return result\n",
    "    return wrapper_fn\n",
    "\n",
    "\n",
    "class HLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HLoss, self).__init__()\n",
    "\n",
    "    def forward(self, x, full=False):\n",
    "        num_data = x.shape[0]\n",
    "        b = F.softmax(x, dim=1) * F.log_softmax(x, dim=1)\n",
    "        if full: \n",
    "            return -1.0 * b.sum(1)\n",
    "        b = -1.0 * b.sum()\n",
    "        b = b/num_data\n",
    "        return b\n",
    "\n",
    "class XeLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XeLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, y, x):\n",
    "        num_data = x.shape[0]\n",
    "        b = F.softmax(y, dim=1)*F.log_softmax(x, dim=1) - F.softmax(y, dim=1)*F.log_softmax(y, dim=1)\n",
    "        b = -1.0 * b.sum()\n",
    "        b = b/num_data\n",
    "        return b\n",
    "    \n",
    "class Jensen_Shannon(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Jensen_Shannon, self).__init__()\n",
    "        \n",
    "    def forward(self, y, x):\n",
    "        num_data = x.shape[0]\n",
    "        b = F.softmax(y, dim=1)*F.log_softmax(x, dim=1) - F.softmax(y, dim=1)*F.log_softmax(y, dim=1)\n",
    "        b += F.softmax(x, dim=1)*F.log_softmax(y, dim=1) - F.softmax(x, dim=1)*F.log_softmax(x, dim=1)\n",
    "        b = -0.5 * b.sum()\n",
    "        b = b/num_data\n",
    "        return b\n",
    "\n",
    "def our_truncnorm(a, b, mu, sigma, x=None, mode='pdf'):\n",
    "    a, b = (a - mu) / sigma, (b - mu) / sigma\n",
    "    if mode=='pdf':\n",
    "        return truncnorm.pdf(x, a, b, loc = mu, scale = sigma)\n",
    "    elif mode=='rvs':\n",
    "        return truncnorm.rvs(a, b, loc = mu, scale = sigma)\n",
    "    \n",
    "def aggregate(features, edge_index, agg_model, num_hop):\n",
    "    n = features.shape[0]\n",
    "    edge_index_w_sl = u.add_self_loops(edge_index, num_nodes = n)[0]\n",
    "    s_vec = agg_model(features, edge_index_w_sl)\n",
    "    return s_vec\n",
    "\n",
    "def log_normal(a, b, sigma):\n",
    "    return -1 * torch.pow(a - b, 2) / (2 * torch.pow(sigma, 2)) #/root2pi / sigma\n",
    "\n",
    "def augment(args, org_edge_index, org_feature, delta_G_e, delta_G_v):\n",
    "    m = org_edge_index.shape[1]\n",
    "    num_edge_drop = int(m*delta_G_e)\n",
    "    #######  flip_edge (A=1)  #######\n",
    "    idx = torch.randperm(m, device='cuda')[:m-num_edge_drop]\n",
    "    aug_edge_index = org_edge_index[:, idx]\n",
    "    #################################    \n",
    "    \n",
    "    n = org_feature.shape[0]\n",
    "    num_node_drop = int(n*delta_G_v)\n",
    "    \n",
    "    aug_feature = org_feature.clone()\n",
    "    node_list = torch.ones(n, 1, device = device)\n",
    "    ##########  flip_feat  ##########\n",
    "    idx = torch.randperm(n, device='cuda')[:num_node_drop]\n",
    "    aug_feature[idx] = 0\n",
    "    node_list[idx] = 0\n",
    "\n",
    "    if num_node_drop:\n",
    "        aug_feature *= n / (n-num_node_drop)\n",
    "    #################################\n",
    "    return aug_edge_index, aug_feature, node_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "hard_xe_loss_op = nn.CrossEntropyLoss()   \n",
    "soft_xe_loss_op = XeLoss()\n",
    "h_loss_op = HLoss()\n",
    "js_loss_op = Jensen_Shannon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'seed': 1234, 'use_seed': True, 'dataset': 'CORA', 'num_epochs': 7000, 'max_epochs': 2000, 'num': 2, 'print': 1, 'wandb': 0, 'wandb_name': 'ours_manyparam', 'model_name': 'GCN', 'emb_dim': 64, 'num_layers': 2, 'lr': 0.005, \n",
    "'decay': 0.0005, 'dropout': 0.5, 'att_dropout': 0.5, 'num_heads': 8, 'a_e': 100, 'b_e': 1, 'a_v': 100, 'b_v': 1, 'kl': 2.0, 'h': 0.2, 'sigma_delta_e': 0.03, 'sigma_delta_v': 0.03, 'mu_e': 0.6, 'mu_v': 0.2, 'lam1_e': 1, 'lam1_v': 1, 'lam2_e': 0.0, 'lam2_v': 0.0, 'option_loss': 0}\n",
    "\n",
    "from collections import namedtuple\n",
    "A = namedtuple('a', a)\n",
    "args = A(**a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.emb_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features, edge_index, train_index, train_label, valid_index, valid_label, test_index, test_label, num_classes = load_nodes(args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data from graphsaint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = None\n",
    "multilabel = True\n",
    "\n",
    "if not os.path.exists('graphsaintdata') and not os.path.exists('data'):\n",
    "    raise ValueError(\"The directory graphsaintdata does not exist!\")\n",
    "elif os.path.exists('graphsaintdata') and not os.path.exists('data'):\n",
    "    os.rename('graphsaintdata', 'data')\n",
    "# prefix = \"data/{}\".format(args.dataset)#################################\n",
    "prefix = \"data/ppi\"#################################\n",
    "DataType = namedtuple('Dataset', ['num_classes', 'train_nid', 'g'])\n",
    "\n",
    "adj_full = scipy.sparse.load_npz('./{}/adj_full.npz'.format(prefix)).astype(bool)#################\n",
    "g = dgl.from_scipy(adj_full)\n",
    "num_nodes = g.num_nodes()\n",
    "\n",
    "adj_train = scipy.sparse.load_npz('./{}/adj_train.npz'.format(prefix)).astype(bool)#############\n",
    "train_nid = np.array(list(set(adj_train.nonzero()[0])))\n",
    "\n",
    "role = json.load(open('./{}/role.json'.format(prefix)))\n",
    "mask = np.zeros((num_nodes,), dtype=bool)\n",
    "train_mask = mask.copy()\n",
    "train_mask[role['tr']] = True\n",
    "val_mask = mask.copy()\n",
    "val_mask[role['va']] = True\n",
    "test_mask = mask.copy()\n",
    "test_mask[role['te']] = True\n",
    "\n",
    "feats = np.load('./{}/feats.npy'.format(prefix))\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(feats[train_nid])\n",
    "feats = scaler.transform(feats)\n",
    "\n",
    "class_map = json.load(open('./{}/class_map.json'.format(prefix)))\n",
    "class_map = {int(k): v for k, v in class_map.items()}\n",
    "\n",
    "if multilabel:\n",
    "    # Multi-label binary classification\n",
    "    num_classes = len(list(class_map.values())[0])\n",
    "    class_arr = np.zeros((num_nodes, num_classes))\n",
    "    for k, v in class_map.items():\n",
    "        class_arr[k] = v\n",
    "else:\n",
    "    num_classes = max(class_map.values()) - min(class_map.values()) + 1\n",
    "    class_arr = np.zeros((num_nodes,))\n",
    "    for k, v in class_map.items():\n",
    "        class_arr[k] = v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Laurelwoods\\AppData\\Local\\Temp\\ipykernel_20388\\5219321.py:6: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
      "  edge_index = torch.Tensor(adj_full.nonzero()).to(torch.int)\n"
     ]
    }
   ],
   "source": [
    "features = torch.tensor(feats, dtype=torch.float)\n",
    "labels = torch.tensor(class_arr, dtype=torch.float if multilabel else torch.long)\n",
    "num_classes = num_classes\n",
    "bn = False\n",
    "\n",
    "edge_index = torch.Tensor(adj_full.nonzero()).to(torch.int)\n",
    "train_index = torch.tensor(train_mask, dtype=torch.bool).tolist()\n",
    "train_label = labels[train_index]\n",
    "valid_index = torch.tensor(val_mask, dtype=torch.bool).tolist()\n",
    "valid_label = labels[valid_index]\n",
    "test_index = torch.tensor(test_mask, dtype=torch.bool).tolist()\n",
    "test_label = labels[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.to(device)\n",
    "edge_index = edge_index.to(device)\n",
    "\n",
    "train_label = train_label.to(device)\n",
    "valid_label = valid_label.to(device)\n",
    "test_label = test_label.to(device)\n",
    "\n",
    "in_channels = features.shape[1]\n",
    "hidden_channels = args.emb_dim\n",
    "num_layers = args.num_layers\n",
    "dropout = args.dropout\n",
    "learning_rate = args.lr\n",
    "weight_decay = args.decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_model = AGG_NET(num_hop = num_layers).cuda()\n",
    "agg_model.eval()\n",
    "with torch.no_grad():\n",
    "    org_ego = aggregate(torch.ones(features.shape[0],1, device = device), edge_index, agg_model,args.num_layers)\n",
    "if args.model_name == 'GCN':\n",
    "    model = GCN(in_channels, hidden_channels, num_classes, num_layers, dropout).to(device)\n",
    "elif args.model_name == 'GAT':\n",
    "    model = GAT(in_channels, hidden_channels, num_classes, num_layers, dropout, args.num_heads, args.att_dropout).to(device)\n",
    "elif args.model_name == 'SAGE':\n",
    "    model = SAGE(in_channels, hidden_channels, num_classes, num_layers, dropout).to(device)\n",
    "elif args.model_name == 'MLP':\n",
    "    model = MLP(in_channels, hidden_channels, num_classes, num_layers, dropout).to(device)\n",
    "opt_model = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)    \n",
    "\n",
    "best_valid_acc, best_valid_f1, best_test_acc, best_test_f1, best_epoch = -1, -1, -1, -1, -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_feature, aug_edge_index, aug_node_list = features, edge_index, torch.ones(features.shape[0], 1, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_node = feature.shape[0]    #############\n",
    "num_node = features.shape[0]    #############\n",
    "num_edge = edge_index.shape[1]\n",
    "\n",
    "delta_G_e = 1 - aug_edge_index.shape[1]/num_edge\n",
    "delta_G_e_aug = our_truncnorm(0, 1, delta_G_e, args.sigma_delta_e, mode='rvs')\n",
    "\n",
    "# delta_G_v = 1 - node_list.sum().item()/num_node ########\n",
    "delta_G_v = 1 - aug_node_list.sum().item()/num_node ########\n",
    "delta_G_v_aug = our_truncnorm(0, 1, delta_G_v, args.sigma_delta_v, mode='rvs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug_edge_index2, aug_feature2, node_list2 = augment(args, edge_index, feature, delta_G_e_aug, delta_G_v_aug)    ####\n",
    "aug_edge_index2, aug_feature2, node_list2 = augment(args, edge_index, features, delta_G_e_aug, delta_G_v_aug)    ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "output = model(features, edge_index)     ###########\n",
    "aug_edge_index = aug_edge_index.to(device)      #############\n",
    "\n",
    "feat_ones = torch.ones(num_node, 1, device = device)\n",
    "with torch.no_grad():\n",
    "    delta_g_e     = 1 - (aggregate(feat_ones,  aug_edge_index,agg_model,  args.num_layers) / org_ego).squeeze(1) \n",
    "    delta_g_aug_e = 1 - (aggregate(feat_ones,  aug_edge_index2,agg_model, args.num_layers) / org_ego).squeeze(1)\n",
    "    delta_g_v     = 1 - (aggregate(aug_node_list,  edge_index,agg_model,      args.num_layers) / org_ego).squeeze(1) ####\n",
    "    delta_g_aug_v = 1 - (aggregate(node_list2, edge_index,agg_model,      args.num_layers) / org_ego).squeeze(1)\n",
    "\n",
    "\n",
    "max_ent = h_loss_op(torch.full((1, output.shape[1]), 1 / output.shape[1])).item()\n",
    "ent = h_loss_op(output.detach(), True) / max_ent\n",
    "\n",
    "\n",
    "p     = args.lam1_e * log_normal(delta_g_e,     args.mu_e, args.a_e * ent + args.b_e) + \\\n",
    "        args.lam1_v * log_normal(delta_g_v,     args.mu_v, args.a_v * ent + args.b_v)\n",
    "p_aug = args.lam1_e * log_normal(delta_g_aug_e, args.mu_e, args.a_e * ent + args.b_e) + \\\n",
    "        args.lam1_v * log_normal(delta_g_aug_v, args.mu_v, args.a_v * ent + args.b_v)\n",
    "\n",
    "q     = np.log(our_truncnorm(0, 1, delta_G_e_aug, args.sigma_delta_e, x=delta_G_e, mode='pdf')) + \\\n",
    "        args.lam2_e * betaln(num_edge - num_edge * delta_G_e + 1, num_edge * delta_G_e + 1) + \\\n",
    "        np.log(our_truncnorm(0, 1, delta_G_v_aug, args.sigma_delta_v, x=delta_G_v, mode='pdf')) + \\\n",
    "        args.lam2_v * betaln(num_node - num_node * delta_G_v + 1, num_node * delta_G_v + 1)\n",
    "q_aug = np.log(our_truncnorm(0, 1, delta_G_e, args.sigma_delta_e, x=delta_G_e_aug, mode='pdf')) + \\\n",
    "        args.lam2_e * betaln(num_edge - num_edge * delta_G_e_aug + 1, num_edge * delta_G_e_aug + 1) + \\\n",
    "        np.log(our_truncnorm(0, 1, delta_G_v, args.sigma_delta_v, x=delta_G_v_aug, mode='pdf')) + \\\n",
    "        args.lam2_v * betaln(num_node - num_node * delta_G_v_aug + 1, num_node * delta_G_v_aug + 1)\n",
    "\n",
    "\n",
    "acceptance = ( (torch.sum(p_aug) - torch.sum(p))  - (q_aug - q) )\n",
    "\n",
    "f1 = 0\n",
    "acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.0884, device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acceptance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
