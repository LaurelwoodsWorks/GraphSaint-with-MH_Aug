{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphsaint.sampler import SAINTNodeSampler, SAINTEdgeSampler, SAINTRandomWalkSampler\n",
    "from graphsaint.config import CONFIG\n",
    "from graphsaint.modules import GCNNet\n",
    "from graphsaint.utils import Logger, evaluate, save_log_dir, load_data, calc_f1\n",
    "import warnings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "graphsaint > tran_sampling.py > main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {\n",
    "        'aggr': 'concat', 'arch': '1-0-1-0', 'dataset': 'ppi', 'dropout': 0, 'edge_budget': 4000, 'length': 2,\n",
    "        'log_dir': 'none', 'lr': 0.01, 'n_epochs': 50, 'n_hidden': 512, 'no_batch_norm': False, 'node_budget': 6000,\n",
    "        'num_subg': 50, 'num_roots': 3000, 'sampler': 'node', 'use_val': True, 'val_every': 1, 'num_workers_sampler': 0,\n",
    "        'num_subg_sampler': 10000, 'batch_size_sampler': 200, 'num_workers': 8, 'full': True, 'online': False, 'gpu': 0,\n",
    "}\n",
    "multilabel =  True\n",
    "\n",
    "from collections import namedtuple\n",
    "A = namedtuple('a', a)\n",
    "args = A(**a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and preprocess dataset\n",
    "data = load_data(args, multilabel)\n",
    "g = data.g\n",
    "train_mask = g.ndata['train_mask']\n",
    "val_mask = g.ndata['val_mask']\n",
    "test_mask = g.ndata['test_mask']\n",
    "labels = g.ndata['label']\n",
    "\n",
    "train_nid = data.train_nid\n",
    "\n",
    "in_feats = g.ndata['feat'].shape[1]\n",
    "n_classes = data.num_classes\n",
    "n_nodes = g.num_nodes()\n",
    "n_edges = g.num_edges()\n",
    "\n",
    "n_train_samples = train_mask.int().sum().item()\n",
    "n_val_samples = val_mask.int().sum().item()\n",
    "n_test_samples = test_mask.int().sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(num_classes=121, train_nid=array([   0,    1,    2, ..., 9713, 9714, 9715]), g=Graph(num_nodes=14755, num_edges=450540,\n",
       "      ndata_schemes={'feat': Scheme(shape=(50,), dtype=torch.float32), 'label': Scheme(shape=(121,), dtype=torch.float32), 'train_mask': Scheme(shape=(), dtype=torch.bool), 'val_mask': Scheme(shape=(), dtype=torch.bool), 'test_mask': Scheme(shape=(), dtype=torch.bool)}\n",
       "      edata_schemes={}))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "        'dn': args.dataset, 'g': g, 'train_nid': train_nid, 'num_workers_sampler': args.num_workers_sampler,\n",
    "        'num_subg_sampler': args.num_subg_sampler, 'batch_size_sampler': args.batch_size_sampler,\n",
    "        'online': args.online, 'num_subg': args.num_subg, 'full': args.full\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.1428573 2.9642859 3.        ... 3.        3.2807019 3.1612902]\n",
      "[0.00028198 0.00228718 0.00027085 ... 0.00036113 0.00128654 0.00033201]\n",
      "The number of subgraphs is:  200\n"
     ]
    }
   ],
   "source": [
    "if args.sampler == \"node\":\n",
    "        saint_sampler = SAINTNodeSampler(args.node_budget, **kwargs)\n",
    "elif args.sampler == \"edge\":\n",
    "    saint_sampler = SAINTEdgeSampler(args.edge_budget, **kwargs)\n",
    "elif args.sampler == \"rw\":\n",
    "    saint_sampler = SAINTRandomWalkSampler(args.num_roots, args.length, **kwargs)\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "loader = DataLoader(saint_sampler, collate_fn=saint_sampler.__collate_fn__, batch_size=1,\n",
    "                    shuffle=True, num_workers=args.num_workers, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels shape: torch.Size([14755, 121])\n",
      "features shape: torch.Size([14755, 50])\n",
      "a(aggr='concat', arch='1-0-1-0', dataset='ppi', dropout=0, edge_budget=4000, length=2, log_dir='none', lr=0.01, n_epochs=50, n_hidden=512, no_batch_norm=False, node_budget=6000, num_subg=50, num_roots=3000, sampler='node', use_val=True, val_every=1, num_workers_sampler=0, num_subg_sampler=10000, batch_size_sampler=200, num_workers=8, full=True, online=False, gpu=0)\n",
      "GPU memory allocated before training(MB) 26.75830078125\n"
     ]
    }
   ],
   "source": [
    "# set device for dataset tensors\n",
    "cpu_flag = False\n",
    "\n",
    "if args.gpu < 0:\n",
    "    cuda = False\n",
    "else:\n",
    "    cuda = True\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "    val_mask = val_mask.cuda()\n",
    "    test_mask = test_mask.cuda()\n",
    "    if not cpu_flag:\n",
    "        g = g.to('cuda:{}'.format(args.gpu))\n",
    "\n",
    "print('labels shape:', g.ndata['label'].shape)\n",
    "print(\"features shape:\", g.ndata['feat'].shape)\n",
    "\n",
    "model = GCNNet(\n",
    "    in_dim=in_feats,\n",
    "    hid_dim=args.n_hidden,\n",
    "    out_dim=n_classes,\n",
    "    arch=args.arch,\n",
    "    dropout=args.dropout,\n",
    "    batch_norm=not args.no_batch_norm,\n",
    "    aggr=args.aggr\n",
    ")\n",
    "\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "\n",
    "# logger and so on\n",
    "log_dir = save_log_dir(args)\n",
    "logger = Logger(os.path.join(log_dir, 'loggings'))\n",
    "logger.write(args)\n",
    "\n",
    "# use optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                lr=args.lr)\n",
    "\n",
    "# set train_nids to cuda tensor\n",
    "if cuda:\n",
    "    train_nid = torch.from_numpy(train_nid).cuda()\n",
    "    print(\"GPU memory allocated before training(MB)\",\n",
    "            torch.cuda.memory_allocated(device=train_nid.device) / 1024 / 1024)\n",
    "start_time = time.time()\n",
    "best_f1 = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Laurelwoods\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dgl\\backend\\pytorch\\tensor.py:445: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n",
      "C:\\Users\\Laurelwoods\\AppData\\Local\\Temp\\ipykernel_1068\\3363985756.py:21: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1/50, Iteration 200/200:training loss 4.100785255432129\n",
      "Train F1-mic 0.9926, Train F1-mac 0.9911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Laurelwoods\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dgl\\backend\\pytorch\\tensor.py:445: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val F1-mic 0.9690, Val F1-mac 0.9644\n",
      "new best val f1: 0.9690338883751792\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m task \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m######## alternative to Argparser\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(args\u001b[39m.\u001b[39mn_epochs):\n\u001b[1;32m----> 4\u001b[0m     \u001b[39mfor\u001b[39;00m j, subg \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(loader):\n\u001b[0;32m      5\u001b[0m         \u001b[39mif\u001b[39;00m cuda:\n\u001b[0;32m      6\u001b[0m             subg \u001b[39m=\u001b[39m subg\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mcurrent_device())\n",
      "File \u001b[1;32mc:\\Users\\Laurelwoods\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:442\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator\n\u001b[0;32m    441\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 442\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_iterator()\n",
      "File \u001b[1;32mc:\\Users\\Laurelwoods\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:388\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 388\u001b[0m     \u001b[39mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Laurelwoods\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1043\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1036\u001b[0m w\u001b[39m.\u001b[39mdaemon \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[39m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[39m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m \u001b[39m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1040\u001b[0m \u001b[39m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m \u001b[39m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m \u001b[39m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1043\u001b[0m w\u001b[39m.\u001b[39;49mstart()\n\u001b[0;32m   1044\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_queues\u001b[39m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1045\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_workers\u001b[39m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mc:\\Users\\Laurelwoods\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m _current_process\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdaemon\u001b[39m\u001b[39m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mdaemonic processes are not allowed to have children\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_Popen(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sentinel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen\u001b[39m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[39m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Laurelwoods\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_context\u001b[39m.\u001b[39;49mget_context()\u001b[39m.\u001b[39;49mProcess\u001b[39m.\u001b[39;49m_Popen(process_obj)\n",
      "File \u001b[1;32mc:\\Users\\Laurelwoods\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\context.py:336\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    334\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    335\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpopen_spawn_win32\u001b[39;00m \u001b[39mimport\u001b[39;00m Popen\n\u001b[1;32m--> 336\u001b[0m     \u001b[39mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[1;32mc:\\Users\\Laurelwoods\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\popen_spawn_win32.py:94\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     reduction\u001b[39m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 94\u001b[0m     reduction\u001b[39m.\u001b[39;49mdump(process_obj, to_child)\n\u001b[0;32m     95\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     96\u001b[0m     set_spawning_popen(\u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Laurelwoods\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdump\u001b[39m(obj, file, protocol\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     ForkingPickler(file, protocol)\u001b[39m.\u001b[39;49mdump(obj)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "task = 1 ######## alternative to Argparser\n",
    "\n",
    "for epoch in range(args.n_epochs):\n",
    "    for j, subg in enumerate(loader):\n",
    "        if cuda:\n",
    "            subg = subg.to(torch.cuda.current_device())\n",
    "        model.train()\n",
    "        # forward\n",
    "        pred = model(subg)\n",
    "        batch_labels = subg.ndata['label']\n",
    "\n",
    "        if multilabel:\n",
    "            loss = F.binary_cross_entropy_with_logits(pred, batch_labels, reduction='sum',\n",
    "                                                        weight=subg.ndata['l_n'].unsqueeze(1))\n",
    "        else:\n",
    "            loss = F.cross_entropy(pred, batch_labels, reduction='none')\n",
    "            loss = (subg.ndata['l_n'] * loss).sum()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
    "        optimizer.step()\n",
    "\n",
    "        if j == len(loader) - 1:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                train_f1_mic, train_f1_mac = calc_f1(batch_labels.cpu().numpy(),\n",
    "                                                        pred.cpu().numpy(), multilabel)\n",
    "                print(f\"epoch:{epoch + 1}/{args.n_epochs}, Iteration {j + 1}/\"\n",
    "                        f\"{len(loader)}:training loss\", loss.item())\n",
    "                print(\"Train F1-mic {:.4f}, Train F1-mac {:.4f}\".format(train_f1_mic, train_f1_mac))\n",
    "    # evaluate\n",
    "    model.eval()\n",
    "    if epoch % args.val_every == 0:\n",
    "        # if cpu_flag and cuda:  # Only when we have shifted model to gpu and we need to shift it back on cpu\n",
    "        #     model = model.to('cpu')\n",
    "        #     val_mask = val_mask.to('cpu')\n",
    "        # model = model.to(torch.cuda.current_device())\n",
    "        labels = labels.to(torch.cuda.current_device()) # ignore\n",
    "        \n",
    "        val_f1_mic, val_f1_mac = evaluate(\n",
    "            model, g, labels, val_mask, multilabel)\n",
    "        print(\n",
    "            \"Val F1-mic {:.4f}, Val F1-mac {:.4f}\".format(val_f1_mic, val_f1_mac))\n",
    "        if val_f1_mic > best_f1:\n",
    "            best_f1 = val_f1_mic\n",
    "            print('new best val f1:', best_f1)\n",
    "            torch.save(model.state_dict(), os.path.join(\n",
    "                log_dir, 'best_model_{}.pkl'.format(task)))\n",
    "        if cpu_flag and cuda:\n",
    "            model.cuda()\n",
    "\n",
    "end_time = time.time()\n",
    "print(f'training using time {end_time - start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
