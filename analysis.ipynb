{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "from collections import namedtuple\n",
    "import scipy.sparse\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import dgl\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from scipy.stats import truncnorm\n",
    "import random\n",
    "\n",
    "from torch.profiler import profile, record_function, ProfilerActivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Juyeong.sampler import SAINTNodeSampler, SAINTEdgeSampler, SAINTRandomWalkSampler\n",
    "from Juyeong.modules import GCNNet, AGGNet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.py\n",
    "def load_data(args, multilabel):\n",
    "    prefix = \"data/{}\".format(args.dataset)\n",
    "    DataType = namedtuple('Dataset', ['num_classes', 'train_nid', 'g'])\n",
    "\n",
    "    adj_full = scipy.sparse.load_npz('./{}/adj_full.npz'.format(prefix)).astype(bool)   # np.bool\n",
    "    g = dgl.from_scipy(adj_full)\n",
    "    num_nodes = g.num_nodes()\n",
    "\n",
    "    adj_train = scipy.sparse.load_npz('./{}/adj_train.npz'.format(prefix)).astype(bool) # np.bool\n",
    "    train_nid = np.array(list(set(adj_train.nonzero()[0])))\n",
    "\n",
    "    role = json.load(open('./{}/role.json'.format(prefix)))\n",
    "    mask = np.zeros((num_nodes,), dtype=bool)\n",
    "    train_mask = mask.copy()\n",
    "    train_mask[role['tr']] = True\n",
    "    val_mask = mask.copy()\n",
    "    val_mask[role['va']] = True\n",
    "    test_mask = mask.copy()\n",
    "    test_mask[role['te']] = True\n",
    "\n",
    "    feats = np.load('./{}/feats.npy'.format(prefix))\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(feats[train_nid])\n",
    "    feats = scaler.transform(feats)\n",
    "\n",
    "    class_map = json.load(open('./{}/class_map.json'.format(prefix)))\n",
    "    class_map = {int(k): v for k, v in class_map.items()}\n",
    "    if multilabel:\n",
    "        # Multi-label binary classification\n",
    "        num_classes = len(list(class_map.values())[0])\n",
    "        class_arr = np.zeros((num_nodes, num_classes))\n",
    "        for k, v in class_map.items():\n",
    "            class_arr[k] = v\n",
    "    else:\n",
    "        num_classes = max(class_map.values()) - min(class_map.values()) + 1\n",
    "        class_arr = np.zeros((num_nodes,))\n",
    "        for k, v in class_map.items():\n",
    "            class_arr[k] = v\n",
    "\n",
    "    g.ndata['feat'] = torch.tensor(feats, dtype=torch.float)\n",
    "    g.ndata['label'] = torch.tensor(class_arr, dtype=torch.float if multilabel else torch.long)\n",
    "    g.ndata['train_mask'] = torch.tensor(train_mask, dtype=torch.bool)\n",
    "    g.ndata['val_mask'] = torch.tensor(val_mask, dtype=torch.bool)\n",
    "    g.ndata['test_mask'] = torch.tensor(test_mask, dtype=torch.bool)\n",
    "\n",
    "    data = DataType(g=g, num_classes=num_classes, train_nid=train_nid)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.py\n",
    "def calc_f1(y_true, y_pred, multilabel):\n",
    "    if multilabel:\n",
    "        y_pred[y_pred > 0] = 1\n",
    "        y_pred[y_pred <= 0] = 0\n",
    "    else:\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "    return f1_score(y_true, y_pred, average=\"micro\"), \\\n",
    "        f1_score(y_true, y_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.py\n",
    "def evaluate(model, g, labels, mask, multilabel=False):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(g)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        f1_mic, f1_mac = calc_f1(labels.cpu().numpy(),\n",
    "                                 logits.cpu().numpy(), multilabel)\n",
    "        return f1_mic, f1_mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from aug.py\n",
    "\n",
    "class HLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HLoss, self).__init__()\n",
    "\n",
    "    def forward(self, x, full=False):\n",
    "        num_data = x.shape[0]\n",
    "        b = F.softmax(x, dim=1) * F.log_softmax(x, dim=1)\n",
    "        if full:\n",
    "            return -1.0 * b.sum(1)\n",
    "        b = -1.0 * b.sum()\n",
    "        b = b / num_data\n",
    "        return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from aug.py\n",
    "\n",
    "class Jensen_Shannon(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Jensen_Shannon, self).__init__()\n",
    "\n",
    "    def forward(self, y, x):\n",
    "        num_data = x.shape[0]\n",
    "        b = F.softmax(y, dim=1) * F.log_softmax(x, dim=1) - F.softmax(y, dim=1) * F.log_softmax(y, dim=1)\n",
    "        b += F.softmax(x, dim=1) * F.log_softmax(y, dim=1) - F.softmax(x, dim=1) * F.log_softmax(x, dim=1)\n",
    "        b = -0.5 * b.sum()\n",
    "        b = b / num_data\n",
    "        return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from aug.py\n",
    "\n",
    "def our_truncnorm(a, b, mu, sigma, x=None, mode='pdf'):\n",
    "    a, b = (a - mu) / sigma, (b - mu) / sigma\n",
    "    if mode=='pdf':\n",
    "        return truncnorm.pdf(x, a, b, loc = mu, scale = sigma)\n",
    "    elif mode=='rvs':\n",
    "        return truncnorm.rvs(a, b, loc = mu, scale = sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from aug.py\n",
    "\n",
    "def aggregate(graph, agg_model):\n",
    "    s_vec = agg_model(graph)\n",
    "    return s_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from aug.py\n",
    "# NOTE: changed import name of torch as torch (from th)\n",
    "\n",
    "def log_normal(a, b, sigma):\n",
    "    return -1 * torch.pow(a - b, 2) / (2 * torch.pow(sigma, 2)) #/root2pi / sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from aug.py\n",
    "# NOTE: changed import name of torch as torch (from th)\n",
    "\n",
    "def augment(g, delta_G_e, delta_G_v):\n",
    "    num_edge_drop = int(g.num_edges() * delta_G_e)\n",
    "    idx = torch.randperm(num_edge_drop, device='cuda:0')[num_edge_drop:]\n",
    "    g.remove_edges(idx)\n",
    "\n",
    "    n = g.num_nodes()\n",
    "    num_node_drop = int(n * delta_G_v)\n",
    "    aug_feature = g.ndata['feat']\n",
    "    node_list = torch.ones(n, 1, device='cuda:0')\n",
    "    idx = torch.randperm(n, device='cuda:0')[:num_node_drop]\n",
    "    aug_feature[idx] = 0\n",
    "    node_list[idx] = 0\n",
    "    if num_node_drop:\n",
    "        aug_feature *= n / (n - num_node_drop)\n",
    "    g.ndata['feat'] = aug_feature\n",
    "\n",
    "    return g, node_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from aug.py\n",
    "# NOTE: changed import name of torch as torch (from th)\n",
    "\n",
    "def generate_aug_graph(g, model,\n",
    "                       sigma_delta_e=0.03, sigma_delta_v=0.03, mu_e=0.6, mu_v=0.2,\n",
    "                       lam1_e=1, lam1_v=1, lam2_e=0.0, lam2_v=0.0,\n",
    "                       a_e=100, b_e=1, a_v=100, b_v=1):\n",
    "    # Original Graph Feature and Metadata Extraction, Preprocessing\n",
    "    num_nodes = g.num_nodes()\n",
    "    num_edges = g.num_edges()\n",
    "\n",
    "    coo_mat = g.edges(form='uv')\n",
    "    coo_mat = torch.tensor([list(coo_mat[0]), list(coo_mat[1])], device='cuda:0')\n",
    "    n_list = torch.ones(num_nodes)\n",
    "\n",
    "    # Create Aggregate Model\n",
    "    agg_model = AGGNet(num_hop=2)\n",
    "    agg_model.cuda()\n",
    "\n",
    "    i = 0\n",
    "    with profile(activities=[ProfilerActivity.CUDA, ProfilerActivity.CPU], record_shapes=True, profile_memory=True) as prof:\n",
    "        with record_function(\"generate_aug_graph-main_loop\"):\n",
    "            while True:   \n",
    "                i += 1\n",
    "\n",
    "                if i == 1000:\n",
    "                    break\n",
    "                ####################################\n",
    "\n",
    "                # Calculate Delta Value\n",
    "                delta_G_e = 1 - coo_mat.shape[1] / num_edges\n",
    "                delta_G_e_aug = our_truncnorm(0, 1, delta_G_e, sigma_delta_e, mode='rvs')\n",
    "\n",
    "                delta_G_v = 1 - n_list.sum().item() / num_nodes\n",
    "                delta_G_v_aug = our_truncnorm(0, 1, delta_G_v, sigma_delta_v, mode='rvs')\n",
    "\n",
    "                # Graph Augmentation According To Delta Value\n",
    "                aug_g, aug_n_list = augment(g, delta_G_e_aug, delta_G_v_aug)\n",
    "                aug_g = dgl.add_self_loop(aug_g)\n",
    "\n",
    "                # message_passing_g = copy.deepcopy(g)\n",
    "                message_passing_g = g.clone()\n",
    "                message_passing_g.ndata['feat'] = torch.ones(num_nodes, 1, device='cuda:0')\n",
    "\n",
    "                # message_passing_aug_g = copy.deepcopy(aug_g)\n",
    "                message_passing_aug_g = aug_g.clone()\n",
    "                message_passing_aug_g.ndata['feat'] = torch.ones(num_nodes, 1, device='cuda:0')\n",
    "\n",
    "                # Calculate ego-graph's message passing value\n",
    "                with torch.no_grad():\n",
    "                    org_ego = aggregate(message_passing_g, agg_model)\n",
    "\n",
    "                # Calculate Augmented Delta Value\n",
    "                with torch.no_grad():\n",
    "                    delta_g_e = 1 - (aggregate(message_passing_g, agg_model) / org_ego).squeeze(1)\n",
    "                    delta_g_aug_e = 1 - (aggregate(message_passing_aug_g, agg_model) / org_ego).squeeze(1)\n",
    "                    delta_g_v = 1 - (aggregate(message_passing_g, agg_model) / org_ego).squeeze(1)\n",
    "                    delta_g_aug_v = 1 - (aggregate(message_passing_aug_g, agg_model) / org_ego).squeeze(1)\n",
    "\n",
    "                # Calculate Target Distribution and Proposal Distribution\n",
    "                h_loss_op = HLoss()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    output = model(g)\n",
    "\n",
    "                max_ent = h_loss_op(torch.full((1, output.shape[1]), 1 / output.shape[1])).item()\n",
    "                ent = h_loss_op(output.detach(), True) / max_ent\n",
    "                \n",
    "                # log_normal: normal distribution에 log를 취한 것\n",
    "                p = lam1_e * log_normal(delta_g_e, mu_e, a_e * ent + b_e) + \\\n",
    "                    0\n",
    "                    # lam1_v * log_normal(delta_g_v, mu_v, a_v * ent + b_v)\n",
    "                p_aug = lam1_e * log_normal(delta_g_aug_e, mu_e, a_e * ent + b_e) + \\\n",
    "                    0\n",
    "                    # lam1_v * log_normal(delta_g_aug_v, mu_v, a_v * ent + b_v)\n",
    "\n",
    "                q = np.log(our_truncnorm(0, 1, delta_G_e_aug, sigma_delta_e, x=delta_G_e, mode='pdf')) + \\\n",
    "                    lam2_e * scipy.special.betaln(num_edges - num_edges * delta_G_e + 1, num_edges * delta_G_e + 1) + \\\n",
    "                    np.log(our_truncnorm(0, 1, delta_G_v_aug, sigma_delta_v, x=delta_G_v, mode='pdf')) + \\\n",
    "                    lam2_v * scipy.special.betaln(num_nodes - num_nodes * delta_G_v + 1, num_nodes * delta_G_v + 1)\n",
    "                q_aug = np.log(our_truncnorm(0, 1, delta_G_e, sigma_delta_e, x=delta_G_e_aug, mode='pdf')) + \\\n",
    "                    lam2_e * scipy.special.betaln(num_edges - num_edges * delta_G_e_aug + 1,\n",
    "                                                num_edges * delta_G_e_aug + 1) + \\\n",
    "                    np.log(our_truncnorm(0, 1, delta_G_v, sigma_delta_v, x=delta_G_v_aug, mode='pdf')) + \\\n",
    "                    lam2_v * scipy.special.betaln(num_nodes - num_nodes * delta_G_v_aug + 1, num_nodes * delta_G_v_aug + 1)\n",
    "\n",
    "                # Calculate Acceptance\n",
    "                acceptance = ((torch.sum(p_aug) - torch.sum(p)) - (q_aug - q))\n",
    "                if np.log(random.random()) < acceptance:\n",
    "                    break\n",
    "    \n",
    "    ###########################################################\n",
    "    print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n",
    "\n",
    "    with open(\"profiler_record-.txt\", 'w') as result:\n",
    "        result.write(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n",
    "    ######################################################\n",
    "\n",
    "\n",
    "    return aug_g, delta_G_e, delta_G_v, delta_G_e_aug, delta_G_v_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into sub functions for profiling.\n",
    "def _generate_aug_graph(g, model,\n",
    "                       sigma_delta_e=0.03, sigma_delta_v=0.03, mu_e=0.6, mu_v=0.2,\n",
    "                       lam1_e=1, lam1_v=1, lam2_e=0.0, lam2_v=0.0,\n",
    "                       a_e=100, b_e=1, a_v=100, b_v=1):\n",
    "    # Original Graph Feature and Metadata Extraction, Preprocessing\n",
    "    def initialize():\n",
    "        num_nodes = g.num_nodes()\n",
    "        num_edges = g.num_edges()\n",
    "\n",
    "        coo_mat = g.edges(form='uv')\n",
    "        coo_mat = torch.tensor([list(coo_mat[0]), list(coo_mat[1])], device='cuda:0')\n",
    "        n_list = torch.ones(num_nodes)\n",
    "\n",
    "        # Create Aggregate Model\n",
    "        agg_model = AGGNet(num_hop=2)\n",
    "        agg_model.cuda()\n",
    "\n",
    "        return num_nodes, num_edges, coo_mat, n_list, agg_model\n",
    "\n",
    "    def calculate_delta_value():\n",
    "        # Calculate Delta Value\n",
    "        delta_G_e = 1 - coo_mat.shape[1] / num_edges\n",
    "        delta_G_e_aug = our_truncnorm(0, 1, delta_G_e, sigma_delta_e, mode='rvs')\n",
    "\n",
    "        delta_G_v = 1 - n_list.sum().item() / num_nodes\n",
    "        delta_G_v_aug = our_truncnorm(0, 1, delta_G_v, sigma_delta_v, mode='rvs')\n",
    "\n",
    "        return delta_G_e, delta_G_e_aug, delta_G_v, delta_G_v_aug  \n",
    "\n",
    "    def graph_augmentation():\n",
    "        # Graph Augmentation According To Delta Value\n",
    "        aug_g, aug_n_list = augment(g, delta_G_e_aug, delta_G_v_aug)\n",
    "        aug_g = dgl.add_self_loop(aug_g)\n",
    "\n",
    "        return aug_g\n",
    "\n",
    "    def message_passing():\n",
    "        # message_passing_g = copy.deepcopy(g)\n",
    "        message_passing_g = g.clone()\n",
    "        message_passing_g.ndata['feat'] = torch.ones(num_nodes, 1, device='cuda:0')\n",
    "\n",
    "        # message_passing_aug_g = copy.deepcopy(aug_g)\n",
    "        message_passing_aug_g = aug_g.clone()\n",
    "        message_passing_aug_g.ndata['feat'] = torch.ones(num_nodes, 1, device='cuda:0')\n",
    "\n",
    "        return message_passing_g, message_passing_aug_g\n",
    "\n",
    "    def calculate_ego_graph_message_passing_value():\n",
    "        # Calculate ego-graph's message passing value\n",
    "        with torch.no_grad():\n",
    "            org_ego = aggregate(message_passing_g, agg_model)\n",
    "\n",
    "        return org_ego\n",
    "\n",
    "    def calculate_augmented_delta():\n",
    "        # Calculate Augmented Delta Value\n",
    "        with torch.no_grad():\n",
    "            delta_g_e = 1 - (aggregate(message_passing_g, agg_model) / org_ego).squeeze(1)\n",
    "            delta_g_aug_e = 1 - (aggregate(message_passing_aug_g, agg_model) / org_ego).squeeze(1)\n",
    "            delta_g_v = 1 - (aggregate(message_passing_g, agg_model) / org_ego).squeeze(1)\n",
    "            delta_g_aug_v = 1 - (aggregate(message_passing_aug_g, agg_model) / org_ego).squeeze(1)\n",
    "\n",
    "        return delta_g_e, delta_g_aug_e, delta_g_v, delta_g_aug_v\n",
    "    \n",
    "    def calculate_distribution():\n",
    "        # Calculate Target Distribution and Proposal Distribution\n",
    "        h_loss_op = HLoss()\n",
    "        return h_loss_op\n",
    "\n",
    "    def compute_model():\n",
    "        with torch.no_grad():\n",
    "            output = model(g)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_ent():\n",
    "        max_ent = h_loss_op(torch.full((1, output.shape[1]), 1 / output.shape[1])).item()\n",
    "        ent = h_loss_op(output.detach(), True) / max_ent\n",
    "\n",
    "        return ent\n",
    "\n",
    "    def compute_p_aug():\n",
    "        # log_normal: normal distribution에 log를 취한 것\n",
    "        p = lam1_e * log_normal(delta_g_e, mu_e, a_e * ent + b_e) + \\\n",
    "            0\n",
    "            # lam1_v * log_normal(delta_g_v, mu_v, a_v * ent + b_v)\n",
    "        p_aug = lam1_e * log_normal(delta_g_aug_e, mu_e, a_e * ent + b_e) + \\\n",
    "            0\n",
    "            # lam1_v * log_normal(delta_g_aug_v, mu_v, a_v * ent + b_v)\n",
    "        \n",
    "        return p, p_aug\n",
    "\n",
    "    def compute_q_aug():\n",
    "        q = np.log(our_truncnorm(0, 1, delta_G_e_aug, sigma_delta_e, x=delta_G_e, mode='pdf')) + \\\n",
    "            lam2_e * scipy.special.betaln(num_edges - num_edges * delta_G_e + 1, num_edges * delta_G_e + 1) + \\\n",
    "            np.log(our_truncnorm(0, 1, delta_G_v_aug, sigma_delta_v, x=delta_G_v, mode='pdf')) + \\\n",
    "            lam2_v * scipy.special.betaln(num_nodes - num_nodes * delta_G_v + 1, num_nodes * delta_G_v + 1)\n",
    "        q_aug = np.log(our_truncnorm(0, 1, delta_G_e, sigma_delta_e, x=delta_G_e_aug, mode='pdf')) + \\\n",
    "            lam2_e * scipy.special.betaln(num_edges - num_edges * delta_G_e_aug + 1,\n",
    "                                        num_edges * delta_G_e_aug + 1) + \\\n",
    "            np.log(our_truncnorm(0, 1, delta_G_v, sigma_delta_v, x=delta_G_v_aug, mode='pdf')) + \\\n",
    "            lam2_v * scipy.special.betaln(num_nodes - num_nodes * delta_G_v_aug + 1, num_nodes * delta_G_v_aug + 1)\n",
    "        \n",
    "        return q, q_aug\n",
    "\n",
    "    #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "    with profile(activities=[ProfilerActivity.CUDA, ProfilerActivity.CPU], record_shapes=True, profile_memory=True) as prof:\n",
    "        with record_function(\"_generate_aug_graph\"):\n",
    "            num_nodes, num_edges, coo_mat, n_list, agg_model = initialize()\n",
    "\n",
    "            i = 0\n",
    "            while True:   \n",
    "                i += 1\n",
    "\n",
    "                if i == 1000:\n",
    "                    break\n",
    "                ####################################\n",
    "\n",
    "                delta_G_e, delta_G_e_aug, delta_G_v, delta_G_v_aug = calculate_delta_value()\n",
    "\n",
    "                aug_g = graph_augmentation()\n",
    "\n",
    "                message_passing_g, message_passing_aug_g = message_passing()\n",
    "\n",
    "                org_ego = calculate_ego_graph_message_passing_value()\n",
    "\n",
    "                delta_g_e, delta_g_aug_e, delta_g_v, delta_g_aug_v = calculate_augmented_delta()\n",
    "\n",
    "                h_loss_op = calculate_distribution()\n",
    "\n",
    "                output = compute_model()   \n",
    "\n",
    "                ent = compute_ent()\n",
    "\n",
    "                p, p_aug = compute_p_aug()\n",
    "\n",
    "                q, q_aug = compute_q_aug()\n",
    "\n",
    "                # Calculate Acceptance\n",
    "                acceptance = ((torch.sum(p_aug) - torch.sum(p)) - (q_aug - q))\n",
    "                if np.log(random.random()) < acceptance:\n",
    "                    break\n",
    "    \n",
    "    ##########################################################\n",
    "    print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n",
    "\n",
    "    with open(\"profiler_record-.txt\", 'w') as result:\n",
    "        result.write(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n",
    "    #####################################################\n",
    "\n",
    "\n",
    "    return aug_g, delta_G_e, delta_G_v, delta_G_e_aug, delta_G_v_aug    # type:ignore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config: 'flickr_n'\n",
    "a = {\n",
    "        'aggr': 'concat', 'arch': '1-1-0', 'dataset': 'flickr', 'dropout': 0.2, 'edge_budget': 6000, 'length': 2,\n",
    "        'log_dir': 'none', 'lr': 0.005, 'decay': 0.0005, 'n_epochs': 50, 'n_hidden': 256, 'no_batch_norm': False, 'node_budget': 8000,\n",
    "        'num_subg': 25, 'num_roots': 6000, 'sampler': 'node', 'use_val': True, 'val_every': 1, 'num_workers_sampler': 0,\n",
    "        'num_subg_sampler': 10000, 'batch_size_sampler': 200, 'num_workers': 8, 'full': False,\n",
    "        'sigma_delta_e': 0.03, 'sigma_delta_v': 0.03, 'mu_e': 0.6, 'mu_v': 0.2, 'lam1_e': 1, 'lam1_v': 1, 'lam2_e': 0.0, 'lam2_v': 0.0,\n",
    "        'a_e': 100, 'b_e': 1, 'a_v': 100, 'b_v': 1, 'kl': 2.0, 'h': 0.2, 'online': False, 'gpu': 0\n",
    "}\n",
    "multilabel =  False\n",
    "\n",
    "from collections import namedtuple\n",
    "A = namedtuple('a', a)\n",
    "args = A(**a)\n",
    "\n",
    "if args.dataset in ['amazon']:\n",
    "        cpu_flag = True\n",
    "else:\n",
    "        cpu_flag = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(args, multilabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(num_classes=7, train_nid=array([    0,     3,     4, ..., 89246, 89248, 89249]), g=Graph(num_nodes=89250, num_edges=899756,\n",
       "      ndata_schemes={'feat': Scheme(shape=(500,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64), 'train_mask': Scheme(shape=(), dtype=torch.bool), 'val_mask': Scheme(shape=(), dtype=torch.bool), 'test_mask': Scheme(shape=(), dtype=torch.bool)}\n",
       "      edata_schemes={}))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Data statistics------'\n",
      "#Nodes 89250\n",
      "#Edges 899756\n",
      "#Classes/Labels (multi binary labels) 7\n",
      "#Train samples 44625\n",
      "#Val samples 22312\n",
      "#Test samples 22313\n"
     ]
    }
   ],
   "source": [
    "# from train_sampling.py\n",
    "g = data.g\n",
    "train_mask = g.ndata['train_mask']\n",
    "val_mask = g.ndata['val_mask']\n",
    "test_mask = g.ndata['test_mask']\n",
    "labels = g.ndata['label']\n",
    "\n",
    "train_nid = data.train_nid\n",
    "\n",
    "in_feats = g.ndata['feat'].shape[1]\n",
    "n_classes = data.num_classes\n",
    "n_nodes = g.num_nodes()\n",
    "n_edges = g.num_edges()\n",
    "\n",
    "n_train_samples = train_mask.int().sum().item()\n",
    "n_val_samples = val_mask.int().sum().item()\n",
    "n_test_samples = test_mask.int().sum().item()\n",
    "\n",
    "print(\"\"\"----Data statistics------'\n",
    "#Nodes %d\n",
    "#Edges %d\n",
    "#Classes/Labels (multi binary labels) %d\n",
    "#Train samples %d\n",
    "#Val samples %d\n",
    "#Test samples %d\"\"\" %\n",
    "        (n_nodes, n_edges, n_classes,\n",
    "        n_train_samples,\n",
    "        n_val_samples,\n",
    "        n_test_samples))\n",
    "# load sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling time: [0.93s]\n",
      "Normalization time: [0.02s]\n",
      "The number of subgraphs is:  200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Laurelwoods\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dgl\\backend\\pytorch\\tensor.py:445: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    }
   ],
   "source": [
    "# from train_sampling.py\n",
    "kwargs = {\n",
    "        'dn': args.dataset, 'g': g, 'train_nid': train_nid, 'num_workers_sampler': args.num_workers_sampler,\n",
    "        'num_subg_sampler': args.num_subg_sampler, 'batch_size_sampler': args.batch_size_sampler,\n",
    "        'online': args.online, 'num_subg': args.num_subg, 'full': args.full}\n",
    "\n",
    "if args.sampler == \"node\":\n",
    "    saint_sampler = SAINTNodeSampler(args.node_budget, **kwargs)\n",
    "elif args.sampler == \"edge\":\n",
    "    saint_sampler = SAINTEdgeSampler(args.edge_budget, **kwargs)\n",
    "elif args.sampler == \"rw\":\n",
    "    saint_sampler = SAINTRandomWalkSampler(args.num_roots, args.length, **kwargs)\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "loader = DataLoader(saint_sampler, collate_fn=saint_sampler.__collate_fn__, batch_size=1,\n",
    "                    shuffle=False, num_workers=args.num_workers, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels shape: torch.Size([89250])\n",
      "features shape: torch.Size([89250, 500])\n"
     ]
    }
   ],
   "source": [
    "# from train_sampling.py\n",
    "cpu_flag = False\n",
    "\n",
    "# set device for dataset tensors\n",
    "if args.gpu < 0:\n",
    "    cuda = False\n",
    "else:\n",
    "    cuda = True\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "    val_mask = val_mask.cuda()\n",
    "    test_mask = test_mask.cuda()\n",
    "    if not cpu_flag:\n",
    "        g = g.to('cuda:{}'.format(args.gpu))\n",
    "\n",
    "print('labels shape:', g.ndata['label'].shape)\n",
    "print(\"features shape:\", g.ndata['feat'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory allocated before training(MB) 195.70556640625\n"
     ]
    }
   ],
   "source": [
    "# from train_sampling.py\n",
    "model = GCNNet(\n",
    "    in_dim=in_feats,\n",
    "    hid_dim=args.n_hidden,\n",
    "    out_dim=n_classes,\n",
    "    arch=args.arch,\n",
    "    dropout=args.dropout,\n",
    "    batch_norm=not args.no_batch_norm,\n",
    "    aggr=args.aggr\n",
    ")\n",
    "\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "\n",
    "# use optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.decay)\n",
    "\n",
    "# set train_nids to cuda tensor\n",
    "if cuda:\n",
    "    train_nid = torch.from_numpy(train_nid).cuda()\n",
    "    print(\"GPU memory allocated before training(MB)\",\n",
    "            torch.cuda.memory_allocated(device=train_nid.device) / 1024 / 1024)\n",
    "start_time = time.time()\n",
    "best_f1 = -1\n",
    "\n",
    "h_loss_op = HLoss()\n",
    "js_loss_op = Jensen_Shannon()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                            Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "    generate_aug_graph-main_loop        44.41%       14.387s       100.00%       32.395s       32.395s       13.018s        40.20%       32.381s       32.381s           0 b    -145.29 Kb      22.55 Mb    -343.48 Gb             1  \n",
      "                        aten::to         0.81%     261.898ms        18.41%        5.963s      57.760us     270.310ms         0.83%        4.190s      40.587us      15.61 Kb           0 b       1.10 Gb           0 b        103242  \n",
      "                  aten::_to_copy         1.97%     638.012ms        17.60%        5.701s     111.896us     309.805ms         0.96%        3.920s      76.934us      15.61 Kb       2.18 Kb       1.10 Gb       2.90 Mb         50952  \n",
      "                     aten::copy_        15.17%        4.915s        15.17%        4.915s      96.906us        3.527s        10.89%        3.527s      69.541us           0 b           0 b           0 b           0 b         50717  \n",
      "                           GSpMM         8.82%        2.858s        10.08%        3.266s     272.397us        3.434s        10.60%        3.726s     310.833us           0 b           0 b      24.25 Gb     433.50 Mb         11988  \n",
      "                      aten::item         0.45%     144.540ms         9.17%        2.971s     118.941us      94.145ms         0.29%        3.290s     131.736us           0 b           0 b           0 b           0 b         24975  \n",
      "       aten::_local_scalar_dense         8.72%        2.826s         8.72%        2.826s     113.994us        3.196s         9.87%        3.196s     128.922us           0 b           0 b           0 b           0 b         24790  \n",
      "                    aten::arange         1.60%     519.136ms         2.87%     929.705ms      20.231us     274.512ms         0.85%     505.810ms      11.007us           0 b           0 b       2.06 Gb     131.66 Mb         45954  \n",
      "                  aten::randperm         0.64%     207.704ms         2.05%     662.901ms     165.891us     180.100ms         0.56%     549.186ms     137.434us           0 b           0 b      58.74 Mb    -103.14 Mb          3996  \n",
      "                       aten::sum         1.12%     363.031ms         1.92%     623.376ms      41.600us     178.248ms         0.55%     364.216ms      24.305us       7.80 Kb       7.80 Kb      32.05 Mb    -445.98 Mb         14985  \n",
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 32.395s\n",
      "Self CUDA time total: 32.381s\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't disable Kineto profiler when it's not running",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     12\u001b[0m     current_subg \u001b[39m=\u001b[39m subg_t[j]\n\u001b[1;32m---> 14\u001b[0m \u001b[39mwith\u001b[39;49;00m profile(activities\u001b[39m=\u001b[39;49m[ProfilerActivity\u001b[39m.\u001b[39;49mCUDA, ProfilerActivity\u001b[39m.\u001b[39;49mCPU], record_shapes\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, profile_memory\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m) \u001b[39mas\u001b[39;49;00m prof:\n\u001b[0;32m     15\u001b[0m     \u001b[39mwith\u001b[39;49;00m record_function(\u001b[39m\"\u001b[39;49m\u001b[39mgenerate_aug_graph\u001b[39;49m\u001b[39m\"\u001b[39;49m):\n\u001b[0;32m     16\u001b[0m         auged_subg, delta_G_e, delta_G_v, delta_G_e_aug, delta_G_v_aug \\\n\u001b[0;32m     17\u001b[0m             \u001b[39m=\u001b[39;49m generate_aug_graph(current_subg, model,\n\u001b[0;32m     18\u001b[0m                                     args\u001b[39m.\u001b[39;49msigma_delta_e, args\u001b[39m.\u001b[39;49msigma_delta_v, args\u001b[39m.\u001b[39;49mmu_e, args\u001b[39m.\u001b[39;49mmu_v,\n\u001b[0;32m     19\u001b[0m                                     args\u001b[39m.\u001b[39;49mlam1_e, args\u001b[39m.\u001b[39;49mlam1_v, args\u001b[39m.\u001b[39;49mlam2_e, args\u001b[39m.\u001b[39;49mlam2_v,\n\u001b[0;32m     20\u001b[0m                                     args\u001b[39m.\u001b[39;49ma_e, args\u001b[39m.\u001b[39;49mb_e, args\u001b[39m.\u001b[39;49ma_v, args\u001b[39m.\u001b[39;49mb_v)\n",
      "File \u001b[1;32mc:\\Users\\Laurelwoods\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\profiler\\profiler.py:509\u001b[0m, in \u001b[0;36mprofile.__exit__\u001b[1;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__exit__\u001b[39m(\u001b[39mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[1;32m--> 509\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstop()\n\u001b[0;32m    510\u001b[0m     prof\u001b[39m.\u001b[39mKinetoStepTracker\u001b[39m.\u001b[39merase_step_count(PROFILER_STEP_NAME)\n",
      "File \u001b[1;32mc:\\Users\\Laurelwoods\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\profiler\\profiler.py:521\u001b[0m, in \u001b[0;36mprofile.stop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecord_steps \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_rec_fn:\n\u001b[0;32m    520\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_rec_fn\u001b[39m.\u001b[39m\u001b[39m__exit__\u001b[39m(\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m--> 521\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transit_action(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcurrent_action, \u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\Laurelwoods\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\profiler\\profiler.py:549\u001b[0m, in \u001b[0;36mprofile._transit_action\u001b[1;34m(self, prev_action, current_action)\u001b[0m\n\u001b[0;32m    547\u001b[0m \u001b[39mif\u001b[39;00m action_list:\n\u001b[0;32m    548\u001b[0m     \u001b[39mfor\u001b[39;00m action \u001b[39min\u001b[39;00m action_list:\n\u001b[1;32m--> 549\u001b[0m         action()\n",
      "File \u001b[1;32mc:\\Users\\Laurelwoods\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\profiler\\profiler.py:139\u001b[0m, in \u001b[0;36m_KinetoProfile.stop_trace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstop_trace\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 139\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprofiler\u001b[39m.\u001b[39;49m\u001b[39m__exit__\u001b[39;49m(\u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\Laurelwoods\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\profiler.py:231\u001b[0m, in \u001b[0;36mprofile.__exit__\u001b[1;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_cuda:\n\u001b[0;32m    230\u001b[0m     torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39msynchronize()\n\u001b[1;32m--> 231\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkineto_results \u001b[39m=\u001b[39m _disable_profiler()\n\u001b[0;32m    232\u001b[0m parsed_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parse_kineto_results(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkineto_results)\n\u001b[0;32m    233\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_events \u001b[39m=\u001b[39m EventList(\n\u001b[0;32m    234\u001b[0m     parsed_results,\n\u001b[0;32m    235\u001b[0m     use_cuda\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_cuda,\n\u001b[0;32m    236\u001b[0m     profile_memory\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofile_memory,\n\u001b[0;32m    237\u001b[0m     with_flops\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwith_flops)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Can't disable Kineto profiler when it's not running"
     ]
    }
   ],
   "source": [
    "subg_t = []\n",
    "\n",
    "for epoch in range(args.n_epochs):\n",
    "    for j, subg in enumerate(loader):\n",
    "        if cuda:\n",
    "            subg = subg.to(torch.cuda.current_device())\n",
    "\n",
    "        # Augment Subgraph\n",
    "        if epoch == 0:\n",
    "            current_subg = subg\n",
    "        else:\n",
    "            current_subg = subg_t[j]\n",
    "\n",
    "        # with profile(activities=[ProfilerActivity.CUDA, ProfilerActivity.CPU], record_shapes=True, profile_memory=True) as prof:\n",
    "        #     with record_function(\"generate_aug_graph\"):\n",
    "        auged_subg, delta_G_e, delta_G_v, delta_G_e_aug, delta_G_v_aug \\\n",
    "            = generate_aug_graph(current_subg, model,\n",
    "                                    args.sigma_delta_e, args.sigma_delta_v, args.mu_e, args.mu_v,\n",
    "                                    args.lam1_e, args.lam1_v, args.lam2_e, args.lam2_v,\n",
    "                                    args.a_e, args.b_e, args.a_v, args.b_v)\n",
    "        if j == 0:\n",
    "            subg_t = []\n",
    "        \n",
    "        subg_t.append(auged_subg)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Profiler didn't finish running",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(prof\u001b[39m.\u001b[39;49mkey_averages()\u001b[39m.\u001b[39mtable(sort_by\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcpu_time_total\u001b[39m\u001b[39m\"\u001b[39m, row_limit\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Laurelwoods\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\profiler\\profiler.py:184\u001b[0m, in \u001b[0;36m_KinetoProfile.key_averages\u001b[1;34m(self, group_by_input_shape, group_by_stack_n)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Averages events, grouping them by operator name and (optionally) input shapes and\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \u001b[39mstack.\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[39m    when creating profiler context manager.\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\n\u001b[1;32m--> 184\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprofiler\u001b[39m.\u001b[39;49mkey_averages(group_by_input_shape, group_by_stack_n)\n",
      "File \u001b[1;32mc:\\Users\\Laurelwoods\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\profiler.py:293\u001b[0m, in \u001b[0;36mprofile.key_averages\u001b[1;34m(self, group_by_input_shape, group_by_stack_n)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mkey_averages\u001b[39m(\u001b[39mself\u001b[39m, group_by_input_shape\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, group_by_stack_n\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[1;32m--> 293\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_finish()\n\u001b[0;32m    294\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_events \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mExpected profiling results\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    295\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_events\u001b[39m.\u001b[39mkey_averages(group_by_input_shape, group_by_stack_n)\n",
      "File \u001b[1;32mc:\\Users\\Laurelwoods\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\profiler.py:253\u001b[0m, in \u001b[0;36mprofile._check_finish\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_finish\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    252\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_events \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 253\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mProfiler didn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt finish running\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Profiler didn't finish running"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"profiler_record-.txt\", 'w') as result:\n",
    "    result.write(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
